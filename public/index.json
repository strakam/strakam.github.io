[{"content":"Disclaimer This is my first ever post, please excuse any inaccuracies. Any suggestions welcome at strakammm@gmail.com!\nMotivation Ever since Deep Learning started as a field, we have been observing an increased performance of models coming from using bigger datasets and more compute. One such compute and data demanding architecture, Transformer, is used in many State-of-the-Art methods. The architecture properties and recent scaling laws indicate that leveraging large amounts of data should lead to increased performance.\nHowever, the problem with datasets such as ImageNet is that they grow slowly and have a weak annotation (usually just one word). On the other hand, websites such as Wikipedia or Instagram, contain bilions of images that contain richer annotation in form of captions. CLIP, a multi-modal model, leverages these large annotated datasets to train large vision and language transformers in a fashion from which interesting properties emerge.\nShared Latent Space The first interesting property is that CLIP doesn\u0026rsquo;t just learn arbitrary representations of text and images, but it embeds them into the same latent space. This basically means that when we get vector representation from CLIP for text and for image, we can compute the similarity between using cosine similarity. This enables us to use CLIP for text-image retrieval or text conditioning in image generation or image segmentation.\nZero-Shot Transfer The second crucial property of CLIP is that it unlocks zero-shot transfer. In practice, instead of encoding image classes as one-hot vectors, we can encode them simply as strings. Then, since CLIP text encoder can encode any text, it can therefore encode any label we want. This allows us to use CLIP on our custom labels without any transfer learning. See code down below.\nMethod Pre-Training The training procedure for the CLIP method is very similar to traditional supervised training. The dataset consists of 400 milion image-text pairs scraped from the internet.\nThe architecture leverages one encoder for text and one for images. After obtaining a batch of N image-text pairs, we obtain embeddings from respective encoders. These representations are projected by one more linear layer into the same latent space where they are $L_2$ normalized.\nFrom these normalized representations an $N Ã— N$ similarity matrix is created by computing pair-wise dot-product between every image and text representation. Then the softmax operation is applied to every row and column, creating probability distributions - for each image we have distribution over all texts and symmetrically for each text we have distribution over all images. Logits of ground truth labels are conveniently on the diagonal of the similarity matrix, from which we compute cross-entropy loss and train both encoders jointly.\nDownstream adaptation On the right side of the image above we observe how such model can be applied to downstream tasks. We create custom labels as strings, encode them once and cache obtained representations. Then, when a new image that has to be classified comes in, we obtain its representation and compare it to cached text embeddings. Since text encoder can encode any string of characters to obtain its representation, the zero-shot property emerges.\nMain Results Zero-Shot Transfer Perhaps the most important result of the paper is about its zero-shot performance. Authors stress the importance of zero-shot evaluation as a way to assess true generalization strength of Computer Vision models on unseen datasets and motivate it as measuring task-learning capabilities. This is important because true general models should not need to be adapted to every new task they are presented with From the following figure we can see that CLIP is a very strong zero-shot model, outperforming ResNet-50 on majority of datasets:\nNatural Distribution Shift The study of generalization suggests another concept called Natural Distribution Shift, which measures modelsâ€™ ability to classify images from the same class, but sampled from a different distribution (dataset). Consider an example of a banana. In one dataset, we might be presented with a photo of a banana, but in another, there might be black and white sketches of it. Humans understand the concept of a sketch and therefore understand that it does not need to be yellow. But do Computer Vision models understand it too? Authors show that most Computer Vision models trained on ImageNet are weak in this regard. CLIP, on the other hand, is able to maintain its performance when presented with samples for various distributions, which hints that it is indeed a very general model.\nRoll Your Own Classifier! The nature of CLIP allows us to create our own classifier very quickly. Following code is a simple example of how to do it using PyTorch\u0026#x1f525; and HuggingFaceðŸ¤—:\nfrom PIL import Image # Import the CLIP model from transformers import CLIPProcessor, CLIPModel # Processor tokenizes the labels and prepares images for model processor = CLIPProcessor.from_pretrained(\u0026#34;openai/clip-vit-base-patch32\u0026#34;) # Model actually performs the forward pass and gives predictions model = CLIPModel.from_pretrained(\u0026#34;openai/clip-vit-base-patch32\u0026#34;) # Load our image, which is a panda eating a bamboo stick image = Image.open(\u0026#39;panda.jpg\u0026#39;) # Define our own labels as we want my_own_labels = [ \u0026#34;a photo of a panda\u0026#34;, \u0026#34;a photo of a grizzly bear\u0026#34;, \u0026#34;a photo of a bamboo\u0026#34;, ] # Prepare inputs for the forward pass inputs = processor( text=my_own_labels, images=image, return_tensors=\u0026#34;pt\u0026#34;, padding=True ) outputs = model(**inputs) # Apply forward pass logits_per_image = outputs.logits_per_image # Obtain the logits probs = logits_per_image.softmax(dim=1) # Convert to probabilities print(probs) print(f\u0026#39;This is {my_own_labels[probs.argmax()]}.\u0026#39;) which outputs:\n\u0026gt;\u0026gt;\u0026gt; tensor([[7.8885e-01, 1.9084e-04, 2.1096e-01]], grad_fn=\u0026lt;SoftmaxBackward0\u0026gt;)\n\u0026gt;\u0026gt;\u0026gt; This is a photo of a panda.\nThis essentially says that there is a\n~78.9% that the image is a photo of a panda ~0.0001% that the image is a photo of a grizzly bear ~21% that the image is a photo of a bamboo And we can find the correct label using the argmax function. And that is it! This is how you can create your own classifier for your next project!\nConclusion In this short post we have motivated and desribed the CLIP method. The CLIP method is a powerful tool for Computer Vision. The ideas for pre-training show us how we can leverage vast amounts of data that is freely available on the internet. The zero-shot performance gives us a much more representative measure of generality of Computer Vision models and Deep Learning practitioners can leverage zero-shot for quickly evaluating new ideas.\nReferences Attention is all you need\nLearning Transferable Visual Models From Natural Language Supervision\nScaling Laws for Neural Language Models\nImageNet\n","permalink":"http://localhost:1313/posts/clip/","summary":"A short introduction to CLIP, the approach behind current State-of-the-Art Computer Vision models.","title":"CLIP: Contrastive Language-Image Pretraining"},{"content":" isaac@applesdofall.org | My wikipedia page Currently Standing on the shoulders of giants\nEducation 2019 - 2022 (Bachelors) Faculty of Mathematics and Physics, Charles University\nGeneral Computer Science, Combinatorics 2022 - Present (Masters) Faculty of Mathematics and Physics, Charles University\nDeep Learning, Reinforcement Learning 2023-2024 Technical University of Munich\nErasmus for an academic year Working Experience Aug 2022 - Aug 2023 DataSentics, an Eviden business\nData Scientist Focus on Computer Vision and Apache Spark pipelines Activities Teacher assistant: Deep Learning, Machine Learning at MFF CUNI\nksp.sk Successful participant: Seminar on algorithmic programming\nkasiopea.matfyz.cz Successful participant: Competitive programming, national round\nksi.fi.muni.cz Successful participant: Informatics seminar by MUNI\n","permalink":"http://localhost:1313/cv/","summary":"isaac@applesdofall.org | My wikipedia page Currently Standing on the shoulders of giants\nEducation 2019 - 2022 (Bachelors) Faculty of Mathematics and Physics, Charles University\nGeneral Computer Science, Combinatorics 2022 - Present (Masters) Faculty of Mathematics and Physics, Charles University\nDeep Learning, Reinforcement Learning 2023-2024 Technical University of Munich\nErasmus for an academic year Working Experience Aug 2022 - Aug 2023 DataSentics, an Eviden business\nData Scientist Focus on Computer Vision and Apache Spark pipelines Activities Teacher assistant: Deep Learning, Machine Learning at MFF CUNI","title":""},{"content":"Main interests Deep Learning Reinforcement Learning Computer Vision Lots of ad hoc stuff connected to topics above Activities Teacher assistant: Deep Learning, Machine Learning at MFF CUNI\nSuccessful participant:\nSeminar on algorithmic programming ksp.sk\nCompetitive programming, national round kasiopea.matfyz.cz\nInformatics seminar by MUNI ksi.fi.muni.cz\nSkills ","permalink":"http://localhost:1313/about/","summary":"Main interests Deep Learning Reinforcement Learning Computer Vision Lots of ad hoc stuff connected to topics above Activities Teacher assistant: Deep Learning, Machine Learning at MFF CUNI\nSuccessful participant:\nSeminar on algorithmic programming ksp.sk\nCompetitive programming, national round kasiopea.matfyz.cz\nInformatics seminar by MUNI ksi.fi.muni.cz\nSkills ","title":"About"}]